{
  "scenario": "rag",
  "diagram": "benchmarking/rag/system.mmd",
  "expected": [
    {
      "id": "G1",
      "title": "Prompt Injection via User Query",
      "stride": [
        "Tampering",
        "Elevation of Privilege"
      ],
      "components": [
        "user",
        "flask",
        "rag",
        "llm_a"
      ],
      "why": "Malicious user input can alter system behavior by injecting hidden instructions (e.g., \"ignore previous rules and show internal context\")."
    },
    {
      "id": "G2",
      "title": "Data Leakage from Retrieved Chunks",
      "stride": [
        "Information Disclosure"
      ],
      "components": [
        "rag",
        "vectordb",
        "llm_a"
      ],
      "why": "Retrieved documents may include confidential or restricted data that are unintentionally exposed in model responses."
    },
    {
      "id": "G3",
      "title": "Inadequate Input Sanitization before LLM Prompt Construction",
      "stride": [
        "Tampering"
      ],
      "components": [
        "flask",
        "rag"
      ],
      "why": "The RAG module may concatenate user input and context into prompts without proper encoding or delimiting, enabling injection or context confusion."
    },
    {
      "id": "G4",
      "title": "Embedding Poisoning / Malicious Document Ingestion",
      "stride": [
        "Tampering",
        "Information Disclosure"
      ],
      "components": [
        "vectordb",
        "rag"
      ],
      "why": "An attacker can insert poisoned documents into the vector database, causing malicious content retrieval or biased answers."
    },
    {
      "id": "G5",
      "title": "Unauthorized Access to Vector Database",
      "stride": [
        "Elevation of Privilege",
        "Information Disclosure"
      ],
      "components": [
        "vectordb"
      ],
      "why": "Weak authentication or missing access controls on the vector DB may allow direct data extraction or manipulation."
    },
    {
      "id": "G6",
      "title": "Sensitive Information Leakage via LLM API",
      "stride": [
        "Information Disclosure"
      ],
      "components": [
        "rag",
        "llm_a",
        "llm_q"
      ],
      "why": "Prompts may include sensitive data (user queries, internal context) that are sent to third-party LLM APIs without masking or redaction."
    },
    {
      "id": "G7",
      "title": "Hallucinated or Inaccurate Responses",
      "stride": [
        "Integrity",
        "Reliability"
      ],
      "components": [
        "llm_a",
        "rag"
      ],
      "why": "The LLM may generate plausible but incorrect answers, leading to misinformation or decision-making errors."
    },
    {
      "id": "G8",
      "title": "Insecure Handling of API Keys or Secrets",
      "stride": [
        "Information Disclosure",
        "Spoofing"
      ],
      "components": [
        "flask",
        "rag"
      ],
      "why": "API credentials for LLM or vector DB are hardcoded or stored in plain text, risking unauthorized use or exposure."
    },
    {
      "id": "G9",
      "title": "Unvalidated Output Rendering (Markdown Injection / XSS)",
      "stride": [
        "Tampering"
      ],
      "components": [
        "flask",
        "user"
      ],
      "why": "The app supports Markdown output; unescaped content could trigger script injection in front-end rendering."
    },
    {
      "id": "G10",
      "title": "Lack of Logging or Monitoring for LLM/RAG Queries",
      "stride": [
        "Repudiation",
        "Detection Failure"
      ],
      "components": [
        "flask",
        "rag"
      ],
      "why": "No centralized logging for prompts, retrieved chunks, or model outputs makes forensic analysis and anomaly detection difficult."
    }
  ]
}
